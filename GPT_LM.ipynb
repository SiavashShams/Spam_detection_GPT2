{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9abce638",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9abce638",
    "outputId": "105fff4a-e048-4701-87fd-69d485fa08c8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b634dc0d330>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Hyperparameters.\n",
    "# I suggest you start with very small values, unless you have a strong PC or are running on the cluster\n",
    "batch_size = 64 # How many independent sequences will we process in parallel?\n",
    "block_size = 128 # What is the maximum context length for predictions?\n",
    "max_iters = 5000 # Max iterations we run the optimization\n",
    "# How often we evaluate across the optimization; every 500 iterations\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "\"\"\"\n",
    "Use 'mps' if on a mac as below:\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\"\"\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# How many batches we use each time we evaluate\n",
    "eval_iters = 200\n",
    "d_model = 384\n",
    "n_head = 6 # This implied that each head has a dimension for the key, query, and values of d_model / 6.\n",
    "n_layer = 6 # This implies we have 6 turns to mix the embeddigs; this is \"Nx\" in the paper\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "FILL_IN = \"FILL_IN\"\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "FONrRI-iKuaB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FONrRI-iKuaB",
    "outputId": "d2720cdc-ff9f-48c0-ad45-ba47ff993031",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1RlmRmXiWVKpZq98ftdtOIdM2lsA1uw3j\n",
      "To: /share/naplab/users/ss6928/NLP/hemingway.txt\n",
      "100%|████████████████████████████████████████| 133k/133k [00:00<00:00, 28.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 'https://drive.google.com/uc?export=download&id=1RlmRmXiWVKpZq98ftdtOIdM2lsA1uw3j'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HXK8qBjo01Yy",
   "metadata": {
    "id": "HXK8qBjo01Yy"
   },
   "source": [
    "As usual, we read the text file and then get two dictionaries from char to idx and in reverse. char embeddings is what we will use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1930b1d3",
   "metadata": {
    "id": "1930b1d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('hemingway.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# Create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # Encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and Test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # First 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bade6f32",
   "metadata": {
    "id": "bade6f32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Randomly select batch_size rows from data's row indices\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    # Select batch_size chuncks of text each of size block_size; stack them\n",
    "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # Do the same for y, but make sure that this is shifted over by 1\n",
    "    yb = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    # I.e. if you select xb (1, 2, 3, 4), yb should be (2, 3, 4, 5)\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    # Each of xb, yb should be (batch_size, block_size)\n",
    "    return xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "938086a6",
   "metadata": {
    "id": "938086a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    # Put the model in eval mode here\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)# Initilize an array of tensor of zeros of size eval_iters\n",
    "        for k in range(eval_iters):\n",
    "            # Get a batch of data\n",
    "            xb, yb = get_batch(split)\n",
    "            # Get the mean and loss\n",
    "            logits,loss = model(xb, yb)\n",
    "            #loss = F.cross_entropy(logits.transpose(1, 2), yb)  \n",
    "            # Get the loss for this batch\n",
    "            losses[k] = loss\n",
    "        # Insert the mean estimate for the loss, based on the slit you are in\n",
    "        out[split] = losses.mean().item()  \n",
    "    # Put the model in train mode here\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54adb41",
   "metadata": {
    "id": "a54adb41"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10e26176",
   "metadata": {
    "id": "10e26176",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents one head of self-attention\n",
    "    Note that since this is a Decoder, this is masked-self-attention\n",
    "    There is no Encoder, so there is no cross-self-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_head):\n",
    "        super().__init__()\n",
    "        # Map each key, query, or value in to a d_head dimensional model.\n",
    "        # Each should be matrices from d_model to d_head\n",
    "        self.W_K = nn.Linear(d_model, d_head)\n",
    "        self.W_Q = nn.Linear(d_model, d_head)\n",
    "        self.W_V = nn.Linear(d_model, d_head)\n",
    "        self.d_head = d_head\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, T, d_model)\n",
    "        # B = batch_size, T = block_size in the below\n",
    "        B,T,d = x.shape\n",
    "        # Get the key and query representations from the embedding x\n",
    "        # (B,T,d_head)\n",
    "        k = self.W_K(x)\n",
    "        # (B,T,d_head)\n",
    "        q = self.W_Q(x)\n",
    "        # (B,T,d_head)\n",
    "        v = self.W_V(x)\n",
    "\n",
    "        # Compute attention scores, and get the new representations for this head\n",
    "\n",
    "        # (B T, d_head) @ (B, d_head, T) = (B, T, T)\n",
    "        # Multiply q by k and divide by the appropriate constant\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "\n",
    "        # (B, T, T)\n",
    "        # Apply a mask to scores, making all scores above the diagonal -inf\n",
    "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # (B, T, T)\n",
    "        # Apply softmax to the final dimension of scores\n",
    "        a =  F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply dropout\n",
    "        a = self.dropout(a)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        # Using a and v, get the new representations\n",
    "        # (B, T, T) @ (B, T, d_head) -> (B, T, d_head)\n",
    "        out = torch.matmul(a, v)\n",
    "        # For each token, return the weighted sum of the values\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple heads of self-attention in parallel\n",
    "    You can have just sequential code below\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, d_head):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(d_head) for _ in range(num_heads)])\n",
    "        # This is to project back to the dimension of d_model. In this case, it is just a learned linear map\n",
    "        self.W_O = nn.Linear(num_heads * d_head, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate the different representations per head along the last dimension\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        concat = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        out = self.W_O(concat)\n",
    "        # Project the concatenation and apply dropout; this is the W_O in \"Attention is all you need\"\n",
    "        out = self.dropout(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "XdEtmrPm7ZCD",
   "metadata": {
    "id": "XdEtmrPm7ZCD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear layer followed by a non-linearity; this is applied at the token level\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        d_ff = 4 * d_model\n",
    "        # Map each token via a linear map to d_ff, apply ReLU, map back to d_model, and then apply dropout\n",
    "        # This can be done with nn.Sequential\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),  # First linear layer: d_model to d_ff\n",
    "            nn.ReLU(),                 # ReLU activation\n",
    "            nn.Linear(d_ff, d_model),  # Second linear layer: d_ff back to d_model\n",
    "            nn.Dropout(dropout)        # Dropout layer\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddb29049",
   "metadata": {
    "id": "ddb29049",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder block: communication followed by computation\n",
    "    These are stacked on top of each other one after another\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        # Each head gets a smaller dimensional representation of the data\n",
    "        # Assume each head gets a representation of dimension d_head and d_model is divisible by n_head\n",
    "        d_head = d_model // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, d_head)\n",
    "        self.ff = FeedFoward(d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is different from the originl transformer paper\n",
    "        In the \"Attention is all you need\" paper, we had\n",
    "        x = self.ln1(x + self.sa(x))\n",
    "        x = self.ln2(x + self.ffwd(x))\n",
    "        See Figure 1 here, and mimic that: https://arxiv.org/pdf/2002.04745.pdf\n",
    "\n",
    "        Here, you can also do:\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \"\"\"\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c230527",
   "metadata": {
    "id": "5c230527",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        # Token embeddings are from vocab_size to d_model\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        # Position embeddings are from block_size (T) to d_model\n",
    "        self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
    "        # This should be n_sequential applications of a DecoderBlock\n",
    "        # This is the \"Nx\" piece in the paper\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(d_model, n_head) for _ in range(n_layer)])\n",
    "         # Final layer norm\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # (B,T,d_model)\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        # (T,d_model)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)).expand(B, -1, -1)\n",
    "        # Add positional encodings to encodings\n",
    "        # (B,T,d_model)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Mix up the token representations over and over via the blocks\n",
    "        # (B,T,d_model)\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply layer norm\n",
    "        # (B,T,d_model)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # Apply the final linear map, to get to dimension vocab_size\n",
    "        # (B,T,vocab_size)\n",
    "        logits = self.ff(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        idx is (B, T) array of indices in the current context\n",
    "        This will generate B total paths in parrallel\n",
    "        We will just geenrate 1 batch below\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            # The model only has kowledge of the context of maximum size block_size\n",
    "            # Get the newest (B, T) data; T = block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # Get the predictions\n",
    "            # (B, T, vocab_size)\n",
    "            logits, loss = self.forward(idx_cond)\n",
    "\n",
    "            # Focus only on the last time step, get the logits\n",
    "            # (B, vocab_size)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            # (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample from the distribution proporttional to probs\n",
    "            # (B, 1)\n",
    "            idx_next = torch.multinomial(probs, 1)\n",
    "\n",
    "            # Append sampled index to the running sequence\n",
    "            # (B, T+1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        self.train()\n",
    "        return idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ThiIDDj1gWse",
   "metadata": {
    "id": "ThiIDDj1gWse",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) / train_loss > self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.tolerance:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sJu3FQkBqT_o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "sJu3FQkBqT_o",
    "outputId": "f6ebbff4-45ce-432c-f2fb-c1ed76fc81f0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 10744382\n",
      "step 0: train loss 4.3611, val loss 4.3620\n",
      "step 500: train loss 1.4713, val loss 1.5302\n",
      "step 1000: train loss 1.1694, val loss 1.3900\n",
      "step 1500: train loss 0.9931, val loss 1.3988\n",
      "We stop at epoch 1500\n"
     ]
    }
   ],
   "source": [
    "model = GPT().to(device)\n",
    "# Print the number of parameters in the model\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "early_stopping = EarlyStopping(tolerance=1, min_delta=0.2)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        if iter:\n",
    "          scheduler.step()\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        early_stopping(losses['train'], losses['val'])\n",
    "        if early_stopping.early_stop:\n",
    "          print(\"We stop at epoch {}\".format(iter))\n",
    "          break\n",
    "\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2037f214",
   "metadata": {
    "id": "2037f214",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"I can love the love of Clear-boats.\"\n",
      "\n",
      "\"Ay,\" the old man said. \"Why do is not bly asket to the hough\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the model with a new line, generate up to 10000 tokens\n",
    "# This is technically doing generations in batches, but here we have a batch size of 1 and 1 element to start in the batch\n",
    "# If you have a model that's very large, d_model = 384, n_head = 6, n_layer = 6, you'll get fairly decent results\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
    "open('fake_hemingway.txt', 'w').write(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "415c080e-c4a2-4ce1-b0f7-826e227cfd86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have enough.\"\n",
      "\n",
      "\"Try are'm?\"\n",
      "\n",
      "Hu boy are enough killed the dolphin,\" the old man said. \"You didn't gave it was not light too swom and would be that it carry beow., Maybe he could with his left hand and his hands on his face and the old man was fast the northing came out of the roje coils with the now edging of the line of there easiboatiful, he thought. And day his eyes will I come not under the tuna your strong.\n",
      "\n",
      "They loggeling circle, the east and when it worked down into the boat, he thought. He was atrength fish strength onside and onto simple.\n",
      "\n",
      "\"That maybe is a fish coppasing,\" the boy said. \"We're baits faither young can and pull of fishermen. I had moved the better and that just fishermen and he sea to Mime to the that he would let it under the rolled the fish off the would broke of his jaws. He sail for hours a a great manyon and the line strength and it some saw The line showed out and he cramp. The closed with the fish for his eye will in the watched lines the was that do now flying for is.\n",
      "\n",
      "Then we would sails about believe the line sixty in the water. They sas no Make him forth and he his goes and he very bad to become. The sun rube down inthe water against the gained. Hey had been a first come spots of reso and the fish's back and eating a much up come. \"They.\"\n",
      "\n",
      "He have the fish ran other sudden of that I.\"\n",
      "\n",
      "The boy're green I say and the this fish riswing and did not even strength. But the sun femorether and was in only spur and Yesully and tery something easily and he saw that the first of the cord and he looked back to up a so much part on it. But then he came again if I get him try in the bow the sun was clear and break into the air, pot nose of the sun and then was fail and the old man at the shado bring up to much beautiful \"I will strange and I do not know.\"\n",
      "\n",
      "\"I baseball?\" the boy said. \"I stay I am clubbing, old man said. \"I'm so holds in very and I chust kill the skiff for that. Then your not have it anything. I must be had him to rise more in the boat and I am. But they am not have hungry. Maybig when I can he had the know that I had can should be him. I worself in the many !And I am never hunger hurt than. You have to surple you not because the line's come too.\"\n",
      "\n",
      "\"A long way bird instrong,\" the old man said. \"Ayon Mose is to kill the surface on his swam. I'm sort. I may eat the harpoon just and great DiMaggio. Just as him when I wonding the had fished line inside the quietly compting away and he knew that brought he had to been more to brace. But there it soon hunger my and the grose six the fish eye and it was some.\n",
      "\n",
      "Alive be he looked up and ate and going bue and put in one you. You do not know it, he thought. It is such as that I can losse to see if he air, old man, wry, a voicking that any my were out.\n",
      "\n",
      "They had killed the bone point the sun and on the skiff that their as went out open and the old man could be reached it. Then he worn he could up see doise with this I east unch for him. The never strong strust and there no maybe and he had that was swimming. And I wish you rest was really bing to keep the line and jump were into a small pill or anainside to strength now. It must try one mine as the hook that he could not get him lash. Then his comn we father handing jumps and flattened for fish and strips on the gear and the old man unbed before he had day so knife out times. This side shark by the bow the old man knew the sun was made fast that can shark from the day. It is he knew that a man and he opened confidencince.\n",
      "\n",
      "\"Now,\" the boy said. \"But the fish is so that you.\"\n",
      "\n",
      "\"An right hand. You did not have slipper of them and their old man's bill and legs on the ocean of the breeze-Ten on get over the line that were dolphin and held it into the noit it come out of such and, no to just it across the suff roaint gently with his eyes. Do you did not fith to ko you.\"\n",
      "\n",
      "He other. One too bother did the sun with the must on his left hand. What albabout he long his fingers. He took at swung the shack that as he laid now as he could see the fish had taken out and drop the bait made a trail made and ssail fast the other hin that many could anCompaning on gently.\n",
      "\n",
      "\"You don't go, old course to old man,\" he said. \"And But I sun't the sun see that he did not see much best edger for me. But I am not raising the fish rongers and the old man said. But the skiff Don't down and slowly and he half was bait only the current jumped that DiMaggio fish what coming he could late his right mouthles strips in he must be going to shark to the bof had and he stopped the sail down and cut the line with his flesh and his shoulders and the fisherman line and the was shark closent and steady.\n",
      "\n",
      "\"What is this had cut,\" he said. \"I'm again, old man,\" he said. \"When you are my boy're big each in two-sleep and love at the skiff and under it.\"\n",
      "\n",
      "\"I'm then with a run trail on the surface of God,\" the boy said. \"He makes done we with thumb a bone point of his wife. He would steady and easil started with a great of there both carried the cop with all shivered with his foot. There weighty were in the air poll strong and he is almost and hung brought he pilm his loved his stay on the negro lift the line and the line line out over on the bow and swam as old man, did not stay not match it was now. They do not know he would pips of his left hand. He had baited himself body again and he put it one his back. There was the old man saw purple then exosphing up the ocean with his jaws usually and stick to long at them part marlin. There is no many jumps and a fisherman was an easy. I must be looked thing it once the part of it. But I don't know.\"\n",
      "\n",
      "I could break the boy and not betore the old man knew he was and this timing so purple or in edge the current, he thought, it will and driver fight cumped it.\n",
      "\n",
      "He first but the straight fish that remember over the great six now give and punched he knew that he could been him and he is thinking of the had backed. There had no different fish to do this strips out because they, were two jaws, finding to green, and staying the boat.\n",
      "\n",
      "The skiff onto the harpoon in order the each were line sea and finger of Chank on the emplent arm and dropped the same line and the trope fast mating, so, that they would like to would lash like the carrying fish had been line line out good and the paint. After if it what DiMaggio to you. I must let to kill? But it is a long bone he said.\"\n",
      "\n",
      "\"There were many for morthber where the brain the fish just come.\"\n",
      "\n",
      "The old man knew that their will let him take it four under between the sun of a cramp. But for was lines to sea and the used off? He could see full because the fish. The old man gaff another old man was in the bow better and he is darkness. It was a voicil sin when he weight of the sun the bone and then he ate open they palm and dropped the current of Afly his bad sinking of his left hand. There he knew his was out eighty-for the day were down and into the strain that each as eighty.\n",
      "\n",
      "Most for commen How he makes this fast and was knifing on his course. For he is more, wish over and the old man knew that to such and sometimes he stew with the slapped his tail up and he felt the skiff and aloud and back to drew for the point in and all of his wifing as ball as he long as the point of his hands and inched on it and you do not good hard into the dark. But I must come in let him up on his boat out. He must be remember on his right hand to would his bird any I have a turn chuon the fish. He attracted the fish mighty was he gone out across it.\n",
      "\n",
      "\"Now,\" the old man said. \"Can I must know make horse, fish, he thought. But I wish had gained up. How defearly to the bow fish. At another he unbed his before the club the fish by looked the boat fishermen and went up and showed down by for the long at the skiff onto the ropped picked to the tail. The boat moved are touching and made between each the fish to with his life her that here hungned at the fish and sometimes but he had was sleeping at the big now when he had conce it. But I did not ever want I have no man to be so much, he thought. I must much my a trick in the line morning to make iis.\n",
      "\n",
      "\"But is moving fananos,\" he said. \"But the sun we'll circle have me open and hard back by and they were wonder for the great fish insteadily. They were not maybe were here knew only I must to get the sun had sail to go the sun. But I don't know the dolphin resuservely so that I can knife, had a floated could passed on the surface and all the fish soart. It could the sun rowly bird to kill the fish though under broke the skiff on the bird. Remp my last was a straight ful for that I drank I can do not usi just strong.\n",
      "\n",
      "\"Keep why do is litte for that it,\" he said. \"Don't make this fish and there is the winds. After hose her was the fishermen, and knows his right hand hands and again and was going the proppess of the bow and the line line with his left hand cut in the bow and the blotches were strong.\n",
      "\n",
      "\"It is,\" he said.\n",
      "\n",
      "For up a moving match and the line wind gutted up around all with I need do. Then, he thought. Make the line so much did not do that I. But think that I can very do not see that cut brise in the tricks. But it too fly fish? Too time in the moing to sea and straight as and as he could feel see that because the line coils of shore does. That known made down was strangeting to the tail. \"I must the boy fish is knife, bad,\" the old man said. \"You're going poush for yellow rick your from there would not but the fish. There was I was swimming him on eightyrd against the boat and settled on the line coils of the hook where old man had dropped into the water and the sun but he came up unleased the steadily and the old man sasking going his jaws could see the line with shark stay up his time. He was builg good would not be good because the fisherman along clubbing come and we on his hands in the air, old man well his fingers and his cout suffering from the water against the bow. Then the surface of the fish was solidity and ag\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))\n",
    "open('fake_hemingway_10000_large_model.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8163ae4f",
   "metadata": {
    "id": "8163ae4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gpt_large.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6bf71-fb40-450a-ba80-2f8338347a16",
   "metadata": {
    "id": "ea4432ff"
   },
   "source": [
    "The larger model generates a more cohesive text. I tried d=96 for smaller model and d=384 for the larger model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb962bb-90d7-46e2-bfe2-79bdd558e7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
